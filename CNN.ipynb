{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d9c9bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet152V2\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import activations\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "261c3484",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/aug_HAM10000_Metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "838dfe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_VAL = 2686\n",
    "NUM_TRN = 26860 - 2686\n",
    "TOTAL = 26860"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ddfe42",
   "metadata": {},
   "source": [
    "(Warning for Sunlight, increased risk of cancer) Actinic keratoses and intraepithelial carcinoma / Bowen's disease (__akiec__), 0\n",
    "\n",
    "(Skin cancer) basal cell carcinoma (__bcc__), 1\n",
    "\n",
    "(HARMLESS) benign keratosis-like lesions (solar lentigines / seborrheic keratoses and lichen-planus like keratoses, __bkl__), 2\n",
    "\n",
    "(HARMLESS) dermatofibroma (__df__),  3\n",
    "\n",
    "melanoma (__mel__),  4\n",
    "\n",
    "(Not entirely harmless) melanocytic nevi (__nv__)   5\n",
    "\n",
    "(HARMLESS) vascular lesions (angiomas, angiokeratomas, pyogenic granulomas and hemorrhage, __vasc__). 6 \n",
    "\n",
    "\n",
    "More than 50% of lesions are confirmed through histopathology (histo), the ground truth for the rest of the cases is either follow-up examination (follow_up), expert consensus (consensus), or confirmation by in-vivo confocal microscopy (confocal). The dataset includes lesions with multiple images, which can be tracked by the lesion_id-column within the HAM10000_metadata file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "544945c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "      <th>dataset</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0027419</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>vidir_modern</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>HAM_0000118</td>\n",
       "      <td>ISIC_0025030</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>vidir_modern</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0026769</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>vidir_modern</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>HAM_0002730</td>\n",
       "      <td>ISIC_0025661</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>80.0</td>\n",
       "      <td>male</td>\n",
       "      <td>scalp</td>\n",
       "      <td>vidir_modern</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>HAM_0001466</td>\n",
       "      <td>ISIC_0031633</td>\n",
       "      <td>bkl</td>\n",
       "      <td>histo</td>\n",
       "      <td>75.0</td>\n",
       "      <td>male</td>\n",
       "      <td>ear</td>\n",
       "      <td>vidir_modern</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    lesion_id      image_id   dx dx_type   age   sex  \\\n",
       "0           0  HAM_0000118  ISIC_0027419  bkl   histo  80.0  male   \n",
       "1           1  HAM_0000118  ISIC_0025030  bkl   histo  80.0  male   \n",
       "2           2  HAM_0002730  ISIC_0026769  bkl   histo  80.0  male   \n",
       "3           3  HAM_0002730  ISIC_0025661  bkl   histo  80.0  male   \n",
       "4           4  HAM_0001466  ISIC_0031633  bkl   histo  75.0  male   \n",
       "\n",
       "  localization       dataset  risk  \n",
       "0        scalp  vidir_modern   0.0  \n",
       "1        scalp  vidir_modern   0.0  \n",
       "2        scalp  vidir_modern   0.0  \n",
       "3        scalp  vidir_modern   0.0  \n",
       "4          ear  vidir_modern   0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89856d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "      <th>dataset</th>\n",
       "      <th>risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25545</td>\n",
       "      <td>25545</td>\n",
       "      <td>HAM_0007622</td>\n",
       "      <td>aug8_ISIC_0027562</td>\n",
       "      <td>akiec</td>\n",
       "      <td>histo</td>\n",
       "      <td>60.0</td>\n",
       "      <td>female</td>\n",
       "      <td>face</td>\n",
       "      <td>rosendahl</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6425</td>\n",
       "      <td>6425</td>\n",
       "      <td>HAM_0005739</td>\n",
       "      <td>ISIC_0029763</td>\n",
       "      <td>nv</td>\n",
       "      <td>follow_up</td>\n",
       "      <td>35.0</td>\n",
       "      <td>male</td>\n",
       "      <td>lower extremity</td>\n",
       "      <td>vidir_molemax</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5018</td>\n",
       "      <td>5018</td>\n",
       "      <td>HAM_0001898</td>\n",
       "      <td>ISIC_0026917</td>\n",
       "      <td>nv</td>\n",
       "      <td>follow_up</td>\n",
       "      <td>50.0</td>\n",
       "      <td>female</td>\n",
       "      <td>abdomen</td>\n",
       "      <td>vidir_molemax</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2572</td>\n",
       "      <td>2572</td>\n",
       "      <td>HAM_0005058</td>\n",
       "      <td>ISIC_0026324</td>\n",
       "      <td>bcc</td>\n",
       "      <td>histo</td>\n",
       "      <td>85.0</td>\n",
       "      <td>female</td>\n",
       "      <td>face</td>\n",
       "      <td>vidir_modern</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25885</td>\n",
       "      <td>25885</td>\n",
       "      <td>HAM_0001302</td>\n",
       "      <td>aug8_ISIC_0032206</td>\n",
       "      <td>akiec</td>\n",
       "      <td>histo</td>\n",
       "      <td>50.0</td>\n",
       "      <td>male</td>\n",
       "      <td>neck</td>\n",
       "      <td>rosendahl</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Unnamed: 0    lesion_id           image_id     dx    dx_type   age  \\\n",
       "0  25545       25545  HAM_0007622  aug8_ISIC_0027562  akiec      histo  60.0   \n",
       "1   6425        6425  HAM_0005739       ISIC_0029763     nv  follow_up  35.0   \n",
       "2   5018        5018  HAM_0001898       ISIC_0026917     nv  follow_up  50.0   \n",
       "3   2572        2572  HAM_0005058       ISIC_0026324    bcc      histo  85.0   \n",
       "4  25885       25885  HAM_0001302  aug8_ISIC_0032206  akiec      histo  50.0   \n",
       "\n",
       "      sex     localization        dataset  risk  \n",
       "0  female             face      rosendahl   2.0  \n",
       "1    male  lower extremity  vidir_molemax   1.0  \n",
       "2  female          abdomen  vidir_molemax   1.0  \n",
       "3  female             face   vidir_modern   3.0  \n",
       "4    male             neck      rosendahl   2.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.sample(frac = 1).reset_index()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3064aeaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['face', 'lower extremity', 'abdomen', 'neck', 'upper extremity',\n",
       "       'trunk', 'back', 'foot', 'chest', 'hand', 'ear', 'scalp',\n",
       "       'unknown', 'genital', 'acral'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['localization'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "093732b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 26860/26860 [00:04<00:00, 5856.76it/s]\n"
     ]
    }
   ],
   "source": [
    "feature_vector = []\n",
    "\n",
    "for i in tqdm(range(0, TOTAL)):\n",
    "    sex = [1, 0, 0]\n",
    "    if df.iloc[i]['sex'] == 'male':\n",
    "        sex = [0, 1, 0]\n",
    "    elif df.iloc[i]['sex'] == 'female':\n",
    "        sex = [0, 0, 1]\n",
    "        \n",
    "    age = df.iloc[i]['age'] / 80\n",
    "    loc = df.iloc[i]['localization']\n",
    "    \n",
    "    feat = np.array(sex)\n",
    "    \n",
    "    if loc == 'abdomen':\n",
    "        feat = np.concatenate((feat, tf.one_hot(0, 15)))# - 0.5))\n",
    "    elif loc == 'scalp':\n",
    "        feat = np.concatenate((feat, tf.one_hot(1, 15)))# - 0.5))\n",
    "    elif loc == 'lower extremity':\n",
    "        feat = np.concatenate((feat, tf.one_hot(2, 15)))# - 0.5))\n",
    "    elif loc == 'trunk':\n",
    "        feat = np.concatenate((feat, tf.one_hot(3, 15)))# - 0.5))\n",
    "    elif loc == 'upper extremity':\n",
    "        feat = np.concatenate((feat, tf.one_hot(4, 15)))# - 0.5))\n",
    "    elif loc == 'back':\n",
    "        feat = np.concatenate((feat,tf.one_hot(5, 15)))# - 0.5))\n",
    "    elif loc == 'neck':\n",
    "        feat = np.concatenate((feat,tf.one_hot(6, 15)))# - 0.5))\n",
    "    elif loc == 'face':\n",
    "        feat = np.concatenate((feat,tf.one_hot(7, 15)))# - 0.5))\n",
    "    elif loc == 'chest':\n",
    "        feat = np.concatenate((feat,tf.one_hot(8, 15)))# - 0.5))\n",
    "    elif loc == 'foot':\n",
    "        feat = np.concatenate((feat,tf.one_hot(9, 15)))# - 0.5))\n",
    "    elif loc == 'ear':\n",
    "        feat = np.concatenate((feat,tf.one_hot(10, 15)))# - 0.5))\n",
    "    elif loc == 'unknown':\n",
    "        feat = np.concatenate((feat,tf.one_hot(11, 15)))# - 0.5))\n",
    "    elif loc == 'hand':\n",
    "        feat = np.concatenate((feat,tf.one_hot(12, 15)))# - 0.5))\n",
    "    elif loc == 'acral':\n",
    "        feat = np.concatenate((feat,tf.one_hot(13, 15)))# - 0.5))\n",
    "    elif loc == 'genital':\n",
    "        feat = np.concatenate((feat,tf.one_hot(14, 15)))# - 0.5))\n",
    "\n",
    "    feature_vector.append(feat)\n",
    "\n",
    "feat_X = np.array(feature_vector)\n",
    "\n",
    "del feature_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e70966",
   "metadata": {},
   "source": [
    "# 1000 Images for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9028ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "earlystop_callback = EarlyStopping(\n",
    "  monitor='val_accuracy', min_delta=0.001,\n",
    "  patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a30f467",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trn = np.array(df['risk'])[:(-1 * NUM_VAL)]\n",
    "y_val = np.array(df['risk'])[(-1 * NUM_VAL):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9980fe9f",
   "metadata": {},
   "source": [
    "# VGG model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bcd1e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                              | 0/26860 [00:00<?, ?it/s]2021-07-08 15:15:13.037194: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-07-08 15:15:13.037346: W tensorflow/core/platform/profile_utils/cpu_utils.cc:126] Failed to get CPU frequency: 0 Hz\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 26860/26860 [26:54<00:00, 16.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# VGG-16\n",
    "preprocessed = []\n",
    "\n",
    "VGG_load = VGG16(weights='imagenet', include_top=False)\n",
    "model = models.Sequential()\n",
    "model.add(VGG_load)\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, TOTAL)):\n",
    "    file = df.iloc[i]['image_id']\n",
    "    feat = feat_X[i]\n",
    "    \n",
    "    img = image.load_img('./Data/HAM10000_images/' + file + '.jpg', target_size= (224,224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = tf.keras.applications.vgg16.preprocess_input(img)\n",
    "    preds = model.predict(img).reshape(25088)\n",
    "\n",
    "    preprocessed.append(np.concatenate((preds, feat)))\n",
    "'''mx = 0\n",
    "for i in range(0, TOTAL):\n",
    "    if mx < np.amax(preprocessed[i]):\n",
    "        mx = np.amax(preprocessed[i])\n",
    "\n",
    "preprocessed = list(preprocessed / mx)\n",
    "'''\n",
    "VGG_X = preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c13b512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG_X_trn = np.array(VGG_X)[:(-1 * NUM_VAL)]\n",
    "\n",
    "VGG_X_val = np.array(VGG_X)[(-1 * NUM_VAL):]\n",
    "\n",
    "del VGG_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97c802b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24174, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_y_trn = []\n",
    "for ele in y_trn:\n",
    "    new_y_trn.append(np.array(tf.one_hot(ele, 4)))\n",
    "    \n",
    "new_y_trn = np.array(new_y_trn)\n",
    "\n",
    "new_y_trn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fa73067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2686, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_y_val = []\n",
    "for ele in y_val:\n",
    "    new_y_val.append(np.array(tf.one_hot(ele, 4)))\n",
    "    \n",
    "new_y_val = np.array(new_y_val)\n",
    "\n",
    "new_y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be6eecc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 24174/24174 [00:04<00:00, 5010.06it/s]\n"
     ]
    }
   ],
   "source": [
    "new_VGG_trn = []\n",
    "\n",
    "for i in tqdm(range(0, NUM_TRN)):\n",
    "    new_VGG_trn.append(np.array(VGG_X_trn[i]))\n",
    "\n",
    "new_VGG_trn = np.array(new_VGG_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb3c5939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 2686/2686 [00:00<00:00, 13108.95it/s]\n"
     ]
    }
   ],
   "source": [
    "new_VGG_val = []\n",
    "\n",
    "for i in tqdm(range(0, NUM_VAL)):\n",
    "    new_VGG_val.append(np.array(VGG_X_val[i]))\n",
    "\n",
    "new_VGG_val = np.array(new_VGG_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e698978b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = models.Sequential()\n",
    "vgg_model.add(layers.Dense(8192, activation='relu'))\n",
    "vgg_model.add(layers.Dense(4096, activation='relu'))\n",
    "vgg_model.add(layers.Dense(2048, activation='relu'))\n",
    "vgg_model.add(layers.Dense(1024, activation='relu'))\n",
    "vgg_model.add(layers.Dense(512, activation='relu'))\n",
    "vgg_model.add(layers.Dense(128, activation='relu'))\n",
    "vgg_model.add(layers.Dense(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5801ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "756/756 [==============================] - 279s 367ms/step - loss: 1.0113 - accuracy: 0.5934 - val_loss: 0.6772 - val_accuracy: 0.7156\n",
      "Epoch 2/30\n",
      "756/756 [==============================] - 281s 371ms/step - loss: 0.4161 - accuracy: 0.8403 - val_loss: 0.4984 - val_accuracy: 0.8075\n",
      "Epoch 3/30\n",
      "756/756 [==============================] - 278s 366ms/step - loss: 0.2031 - accuracy: 0.9255 - val_loss: 0.5400 - val_accuracy: 0.8090\n",
      "Epoch 4/30\n",
      "756/756 [==============================] - 287s 380ms/step - loss: 0.1137 - accuracy: 0.9606 - val_loss: 0.6165 - val_accuracy: 0.8198\n",
      "Epoch 5/30\n",
      "756/756 [==============================] - 279s 368ms/step - loss: 0.0862 - accuracy: 0.9701 - val_loss: 0.6986 - val_accuracy: 0.8213\n",
      "Epoch 6/30\n",
      "756/756 [==============================] - 277s 365ms/step - loss: 0.0699 - accuracy: 0.9759 - val_loss: 0.7651 - val_accuracy: 0.8239\n",
      "Epoch 7/30\n",
      "756/756 [==============================] - 276s 365ms/step - loss: 0.0488 - accuracy: 0.9827 - val_loss: 0.6970 - val_accuracy: 0.8291\n",
      "Epoch 8/30\n",
      "756/756 [==============================] - 283s 372ms/step - loss: 0.0432 - accuracy: 0.9862 - val_loss: 0.6976 - val_accuracy: 0.8109\n",
      "Epoch 9/30\n",
      "149/756 [====>.........................] - ETA: 3:40 - loss: 0.0283 - accuracy: 0.9915"
     ]
    }
   ],
   "source": [
    "vgg_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005),\n",
    "              loss=tf.nn.softmax_cross_entropy_with_logits,\n",
    "              metrics=['accuracy'])\n",
    "vgg_model.fit(new_VGG_trn, new_y_trn, \n",
    "              epochs=30, \n",
    "              validation_data=(new_VGG_val, new_y_val), \n",
    "              callbacks=[earlystop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742f8948",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y_vgg_val = []\n",
    "\n",
    "temp_vgg_val = vgg_model.predict(new_VGG_val)\n",
    "del new_VGG_val\n",
    "\n",
    "temp_vgg_trn = vgg_model.predict(new_VGG_trn)\n",
    "del new_VGG_trn\n",
    "\n",
    "for i in tqdm(range(0, NUM_VAL)):\n",
    "    pred = np.argmax(temp_vgg_val[i])\n",
    "    predict_y_vgg_val.append(pred)\n",
    "    print(str(temp_vgg_val[i]) + \" : \" + pred)\n",
    "    \n",
    "del temp_vgg_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bfbfae",
   "metadata": {},
   "source": [
    "# ResNet setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c4794f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████████████▋                                                                                    | 3993/26860 [05:34<31:58, 11.92it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/18/lbcwrdqx01l27lrh8q3rxlqh0000gn/T/ipykernel_89172/1540489056.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet_v2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100352\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mpreprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/m1tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1629\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/m1tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/m1tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/miniforge3/envs/m1tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/m1tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniforge3/envs/m1tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/m1tf/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ResNet\n",
    "preprocessed = []\n",
    "\n",
    "resnet_load = ResNet152V2(weights='imagenet', include_top=False)\n",
    "model = models.Sequential()\n",
    "model.add(resnet_load)\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "\n",
    "for i in tqdm(range(0, TOTAL)):\n",
    "    file = df.iloc[i]['image_id']\n",
    "    feat = feat_X[i]    \n",
    "    \n",
    "    img = image.load_img('./Data/HAM10000_images/' + file + '.jpg', target_size= (224,224))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = tf.keras.applications.resnet_v2.preprocess_input(img)\n",
    "    preds = model.predict(img).reshape(100352)\n",
    "\n",
    "    preprocessed.append(np.concatenate((preds, feat)))\n",
    "\n",
    "\n",
    "\n",
    "'''mx = 0\n",
    "for i in range(0, TOTAL):\n",
    "    if mx < np.amax(preprocessed[i]):\n",
    "        mx = np.amax(preprocessed[i])\n",
    "\n",
    "preprocessed = list(preprocessed / mx)\n",
    "'''\n",
    "\n",
    "RES_X = preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40645b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "RES_X_trn = np.array(RES_X)[:(-1 * NUM_VAL)]\n",
    "\n",
    "RES_X_val = np.array(RES_X)[(-1 * NUM_VAL):]\n",
    "\n",
    "del RES_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1274a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_RES_trn = []\n",
    "\n",
    "for i in tqdm(range(0, NUM_TRN)):\n",
    "    new_RES_trn.append(np.array(RES_X_trn[i]))\n",
    "\n",
    "new_RES_trn = np.array(new_RES_trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1758e093",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_RES_val = []\n",
    "\n",
    "for i in tqdm(range(0, NUM_VAL)):\n",
    "    new_RES_val.append(np.array(RES_X_val[i]))\n",
    "\n",
    "new_RES_val = np.array(new_RES_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3646e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_model = models.Sequential()\n",
    "#res_model.add(layers.Dense(8192, activation='relu'))\n",
    "res_model.add(layers.Dense(4096, activation='relu'))\n",
    "#res_model.add(layers.Dense(2048, activation='relu'))\n",
    "res_model.add(layers.Dense(1024, activation='relu'))\n",
    "#res_model.add(layers.Dense(512, activation='relu'))\n",
    "res_model.add(layers.Dense(128, activation='relu'))\n",
    "res_model.add(layers.Dense(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b87fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005),\n",
    "              loss=tf.nn.softmax_cross_entropy_with_logits,\n",
    "              metrics=['accuracy'])\n",
    "res_model.fit(new_RES_trn, new_y_trn, \n",
    "              epochs=30, callbacks=[earlystop_callback],\n",
    "              validation_data=(new_RES_val, new_y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001d8f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y_res_val = []\n",
    "\n",
    "temp_res_val = res_model.predict(new_RES_val)\n",
    "del new_RES_val\n",
    "\n",
    "temp_res_trn = res_model.predict(new_RES_trn)\n",
    "del new_RES_trn\n",
    "\n",
    "for i in tqdm(range(0, NUM_VAL)):\n",
    "    pred = np.argmax(temp_res_val[i])\n",
    "    predict_y_res_val.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f474e60b",
   "metadata": {},
   "source": [
    "# Inception V3 setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b1468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inception\n",
    "preprocessed = []\n",
    "\n",
    "IV3_load = InceptionV3(weights='imagenet', include_top=False)\n",
    "model = models.Sequential()\n",
    "model.add(IV3_load)\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "for i in tqdm(range(0, TOTAL)):\n",
    "    file = df.iloc[i]['image_id']\n",
    "    feat = feat_X[i]\n",
    "    \n",
    "    img = image.load_img('./Data/HAM10000_images/' + file + '.jpg', target_size= (299,299))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    preds = model.predict(img).reshape(131072)\n",
    "\n",
    "    preprocessed.append(np.concatenate((preds, feat)))\n",
    "\n",
    "\n",
    "    \n",
    "'''mx = 0\n",
    "for i in range(0, TOTAL):\n",
    "    if mx < np.amax(preprocessed[i]):\n",
    "        mx = np.amax(preprocessed[i])\n",
    "\n",
    "preprocessed = list(preprocessed / mx)\n",
    "'''\n",
    "IV3_X = preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f3abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IV3_X_trn = np.array(IV3_X)[:(-1 * NUM_VAL)]\n",
    "\n",
    "IV3_X_val = np.array(IV3_X)[(-1 * NUM_VAL):]\n",
    "\n",
    "del IV3_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a49d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_IV3_trn = []\n",
    "\n",
    "for i in tqdm(range(0, NUM_TRN)):\n",
    "    new_IV3_trn.append(np.array(IV3_X_trn[i]))\n",
    "\n",
    "new_IV3_trn = np.array(new_IV3_trn)\n",
    "\n",
    "new_IV3_trn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68fdfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_IV3_val = []\n",
    "\n",
    "for i in tqdm(range(0, NUM_VAL)):\n",
    "    new_IV3_val.append(np.array(IV3_X_val[i]))\n",
    "\n",
    "new_IV3_val = np.array(new_IV3_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8308fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "iv3_model = models.Sequential()\n",
    "#iv3_model.add(layers.Dense(8192, activation='relu'))\n",
    "iv3_model.add(layers.Dense(1024, activation='relu'))\n",
    "#iv3_model.add(layers.Dense(2048, activation='relu'))\n",
    "iv3_model.add(layers.Dense(256, activation='relu'))\n",
    "#iv3_model.add(layers.Dense(512, activation='relu'))\n",
    "#iv3_model.add(layers.Dense(128, activation='relu'))\n",
    "iv3_model.add(layers.Dense(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b6f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "iv3_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005),\n",
    "              loss=tf.nn.softmax_cross_entropy_with_logits,\n",
    "              metrics=['accuracy'])\n",
    "iv3_model.fit(new_IV3_trn, new_y_trn, \n",
    "              epochs=30, callbacks=[earlystop_callback],\n",
    "              validation_data=(new_IV3_val, new_y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe31283",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y_iv3_val = []\n",
    "\n",
    "temp_iv3_val = iv3_model.predict(new_IV3_val)\n",
    "del new_IV3_val\n",
    "\n",
    "temp_iv3_trn = iv3_model.predict(new_IV3_trn)\n",
    "del new_IV3_trn\n",
    "\n",
    "for i in tqdm(range(0, NUM_VAL)):\n",
    "    pred = np.argmax(temp_iv3_val[i])\n",
    "    predict_y_iv3_val.append(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd322221",
   "metadata": {},
   "source": [
    "# Making the confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47d311ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_y_iv3_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/18/lbcwrdqx01l27lrh8q3rxlqh0000gn/T/ipykernel_89172/1244506652.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcm_iv3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_y_iv3_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'predict_y_iv3_val' is not defined"
     ]
    }
   ],
   "source": [
    "cm_iv3 = tf.math.confusion_matrix(np.array(y_val), np.array(predict_y_iv3_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1106aeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_vgg = tf.math.confusion_matrix(np.array(y_val), np.array(predict_y_vgg_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e941c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_res = tf.math.confusion_matrix(np.array(y_val), np.array(predict_y_res_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7fcd7306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cm_iv3 = np.array(cm_iv3).astype('float32')\n",
    "cm_vgg = np.array(cm_vgg).astype('float32')\n",
    "#cm_res = np.array(cm_res).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40d6d055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncm_res[0] = cm_res[0] / (1.0 * cm_res[0].sum())\\ncm_res[1] = cm_res[1] / (1.0 * cm_res[1].sum())\\ncm_res[2] = cm_res[2] / (1.0 * cm_res[2].sum())\\ncm_res[3] = cm_res[3] / (1.0 * cm_res[3].sum())\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "cm_iv3[0] = cm_iv3[0] / (1.0 * cm_iv3[0].sum())\n",
    "cm_iv3[1] = cm_iv3[1] / (1.0 * cm_iv3[1].sum())\n",
    "cm_iv3[2] = cm_iv3[2] / (1.0 * cm_iv3[2].sum())\n",
    "cm_iv3[3] = cm_iv3[3] / (1.0 * cm_iv3[3].sum())\n",
    "'''\n",
    "\n",
    "cm_vgg[0] = cm_vgg[0] / (1.0 * cm_vgg[0].sum())\n",
    "cm_vgg[1] = cm_vgg[1] / (1.0 * cm_vgg[1].sum())\n",
    "cm_vgg[2] = cm_vgg[2] / (1.0 * cm_vgg[2].sum())\n",
    "cm_vgg[3] = cm_vgg[3] / (1.0 * cm_vgg[3].sum())\n",
    "\n",
    "'''\n",
    "cm_res[0] = cm_res[0] / (1.0 * cm_res[0].sum())\n",
    "cm_res[1] = cm_res[1] / (1.0 * cm_res[1].sum())\n",
    "cm_res[2] = cm_res[2] / (1.0 * cm_res[2].sum())\n",
    "cm_res[3] = cm_res[3] / (1.0 * cm_res[3].sum())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9aaef500",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm_iv3, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e3c9334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(cm_vgg, annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f250af",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm_res, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167c9b65",
   "metadata": {},
   "source": [
    "# Stacking the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de08b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iv3, max_vgg, max_res = 0, 0, 0\n",
    "\n",
    "max_iv3 = np.amax([np.amax(temp_iv3_trn), np.amax(temp_iv3_val)])\n",
    "max_vgg = np.amax([np.amax(temp_vgg_trn), np.amax(temp_vgg_val)])\n",
    "max_res = np.amax([np.amax(temp_res_trn), np.amax(temp_res_val)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056b13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_trn = []\n",
    "\n",
    "for i in tqdm(range(0, NUM_TRN)):\n",
    "    feat = np.concatenate(( (temp_iv3_trn[i] / max_iv3), \n",
    "                            (temp_vgg_trn[i] / max_vgg), \n",
    "                            (temp_res_trn[i] / max_res) ))\n",
    "    new_X_trn.append(feat)\n",
    "    \n",
    "new_X_trn = np.array(new_X_trn)\n",
    "new_X_trn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48605e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_vgg_val.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6193d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_val = []\n",
    "\n",
    "for i in tqdm(range(0, NUM_VAL)):\n",
    "    feat = np.concatenate(( (temp_iv3_val[i]), \n",
    "                            (temp_vgg_val[i]), \n",
    "                            (temp_res_val[i] ))\n",
    "    new_X_val.append(feat)\n",
    "    \n",
    "new_X_val = np.array(new_X_val)\n",
    "new_X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e19993",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = models.Sequential()\n",
    "\n",
    "final_model.add(layers.Dense(64, activation='relu'))\n",
    "final_model.add(layers.Dense(32, activation='relu'))\n",
    "final_model.add(layers.Dense(16, activation='relu'))\n",
    "final_model.add(layers.Dense(8, activation='relu'))\n",
    "final_model.add(layers.Dense(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef8f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.compile(optimizer='Adam',\n",
    "              loss=tf.nn.softmax_cross_entropy_with_logits,\n",
    "              metrics=['accuracy'])\n",
    "final_model.fit(new_X_trn, new_y_trn, \n",
    "                epochs=10, callbacks=[earlystop_callback],\n",
    "                validation_data=(new_X_val, new_y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdd3b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y_final_val = []\n",
    "temp_final_val = final_model.predict(new_X_val)\n",
    "\n",
    "for i in tqdm(range(0, NUM_VAL)):\n",
    "    pred = np.argmax(temp_final_val[i])\n",
    "    predict_y_final_val.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0661483",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_final = tf.math.confusion_matrix(np.array(df['risk'])[(-1 * NUM_VAL):], np.array(predict_y_final_val))\n",
    "\n",
    "cm_final = np.array(cm_final).astype('float32')\n",
    "\n",
    "cm_final[0] = cm_final[0] / (1.0 * cm_final[0].sum())\n",
    "cm_final[1] = cm_final[1] / (1.0 * cm_final[1].sum())\n",
    "cm_final[2] = cm_final[2] / (1.0 * cm_final[2].sum())\n",
    "cm_final[3] = cm_final[3] / (1.0 * cm_final[3].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf32f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm_final, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c08ae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c68def8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf89b7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3663b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a575bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d69737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de97e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86574dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "102eabe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6388682055100521"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(0, NUM_VAL):\n",
    "    if(y_val[i] == predict_y_vgg_val[i]):\n",
    "        count += 1\n",
    "count / NUM_VAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709220da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c830bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d14aa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e12f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a13dd5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea8324b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3923c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca237e14",
   "metadata": {},
   "source": [
    "# New Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a3417f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/HAM10000_Metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effce3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac = 1).reset_index()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61549d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk = []\n",
    "\n",
    "for i in range(0, TOTAL):\n",
    "    if df.iloc[i]['dx'] in ['bkl', 'df', 'vasc']:\n",
    "        risk.append(0.0)\n",
    "    elif df.iloc[i]['dx'] == 'nv':\n",
    "        risk.append(1.0)\n",
    "    elif df.iloc[i]['dx'] == 'akiec':\n",
    "        risk.append(2.0)\n",
    "    elif df.iloc[i]['dx'] in ['mel', 'bcc']:\n",
    "        risk.append(3.0)\n",
    "        \n",
    "df['risk'] = risk\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a92062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c245ac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4,1, figsize=(15,30), sharex=True)\n",
    "\n",
    "sns.histplot(x='localization', data=df[df['risk'] == 0.0], color='green', stat='probability', ax=axs[0]).set_title('risk 0')\n",
    "sns.histplot(x='localization', data=df[df['risk'] == 1.0], color='yellow', stat='probability', ax=axs[1]).set_title('risk 1')\n",
    "sns.histplot(x='localization', data=df[df['risk'] == 2.0], color='orange', stat='probability', ax=axs[2]).set_title('risk 2')\n",
    "sns.histplot(x='localization', data=df[df['risk'] == 3.0], color='red', stat='probability', ax=axs[3]).set_title('risk 3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8907408",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "\n",
    "for i in tqdm(range(0, TOTAL)):\n",
    "    file = df.iloc[i]['image_id']\n",
    "    \n",
    "    img = image.load_img('./Data/HAM10000_images/' + file + '.jpg', target_size= (150,200))\n",
    "    img = image.img_to_array(img) / 255.0\n",
    "    \n",
    "    \n",
    "    X.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fd32a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8202bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk = []\n",
    "\n",
    "for ele in df['risk']:\n",
    "    risk.append(tf.one_hot(int(ele), 4))\n",
    "    \n",
    "y = np.array(risk)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fee77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_model = models.Sequential()\n",
    "ori_model.add(layers.Conv2D(64, (3, 3), activation='tanh', input_shape=(150,200,3)))\n",
    "ori_model.add(layers.BatchNormalization())\n",
    "ori_model.add(layers.Activation(activations.elu))\n",
    "ori_model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "\n",
    "ori_model.add(layers.Conv2D(32, (3, 3), activation='tanh'))\n",
    "ori_model.add(layers.BatchNormalization())\n",
    "ori_model.add(layers.Activation(activations.elu))\n",
    "ori_model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "\n",
    "ori_model.add(layers.Conv2D(16, (3, 3), activation='tanh'))\n",
    "ori_model.add(layers.BatchNormalization())\n",
    "ori_model.add(layers.Activation(activations.elu))\n",
    "ori_model.add(layers.MaxPooling2D(2,2))\n",
    "\n",
    "\n",
    "ori_model.add(layers.Flatten(name=\"feature_output\"))\n",
    "\n",
    "ori_model.add(layers.Dense(1024, activation='relu'))\n",
    "ori_model.add(layers.Dense(256, activation='relu'))\n",
    "ori_model.add(layers.Dense(64, activation='relu'))\n",
    "ori_model.add(layers.Dense(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d7536",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_model.compile(optimizer='Adam',\n",
    "              loss=tf.nn.softmax_cross_entropy_with_logits,\n",
    "              metrics=['accuracy'])\n",
    "ori_model.fit(X, y, epochs=25, validation_split=0.1, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5119ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_model.save('./models/feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6690ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(\"./models/feature\")\n",
    "\n",
    "\n",
    "feature_extractor = tf.keras.Model(\n",
    "    inputs=loaded_model.inputs,\n",
    "    outputs=loaded_model.get_layer(name=\"feature_output\").output,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f93869",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = []\n",
    "\n",
    "for i in tqdm(range(0, TOTAL)):\n",
    "    sex = 0 if df.iloc[i]['sex'] == 'male' else 1\n",
    "    age = df.iloc[i]['age']\n",
    "    loc = df.iloc[i]['localization']\n",
    "    \n",
    "    feat = np.array([sex, age])\n",
    "    \n",
    "    if loc == 'abdomen':\n",
    "        feat = np.concatenate((feat, tf.one_hot(0, 15)))\n",
    "    elif loc == 'scalp':\n",
    "        feat = np.concatenate((feat, tf.one_hot(1, 15)))\n",
    "    elif loc == 'lower extremity':\n",
    "        feat = np.concatenate((feat, tf.one_hot(2, 15)))\n",
    "    elif loc == 'trunk':\n",
    "        feat = np.concatenate((feat, tf.one_hot(3, 15)))\n",
    "    elif loc == 'upper extremity':\n",
    "        feat = np.concatenate((feat, tf.one_hot(4, 15)))\n",
    "    elif loc == 'back':\n",
    "        feat = np.concatenate((feat,tf.one_hot(5, 15)))\n",
    "    elif loc == 'neck':\n",
    "        feat = np.concatenate((feat,tf.one_hot(6, 15)))\n",
    "    elif loc == 'face':\n",
    "        feat = np.concatenate((feat,tf.one_hot(7, 15)))\n",
    "    elif loc == 'chest':\n",
    "        feat = np.concatenate((feat,tf.one_hot(8, 15)))\n",
    "    elif loc == 'foot':\n",
    "        feat = np.concatenate((feat,tf.one_hot(9, 15)))\n",
    "    elif loc == 'ear':\n",
    "        feat = np.concatenate((feat,tf.one_hot(10, 15)))\n",
    "    elif loc == 'unknown':\n",
    "        feat = np.concatenate((feat,tf.one_hot(11, 15)))\n",
    "    elif loc == 'hand':\n",
    "        feat = np.concatenate((feat,tf.one_hot(12, 15)))\n",
    "    elif loc == 'acral':\n",
    "        feat = np.concatenate((feat,tf.one_hot(13, 15)))\n",
    "    elif loc == 'genital':\n",
    "        feat = np.concatenate((feat,tf.one_hot(14, 15)))\n",
    "\n",
    "    feature_vector.append(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d5b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = np.array(feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3013ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b197306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = feature_extractor(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd43b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for img_feat, feat in zip(image_features, feature_vector):\n",
    "    X.append(np.concatenate((img_feat, feat)))\n",
    "\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94dcf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0df2c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='Adam',\n",
    "              loss=tf.nn.softmax_cross_entropy_with_logits,\n",
    "              metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=25, validation_split=0.1, batch_size=16, sample_weight=sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbd7666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675306d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c933e1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186014f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ef2a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9ddfa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb2d761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ab873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d8b6ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f79c73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c6a057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd864c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61ce2ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2029e44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/HAM10000_Metadata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efd391a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac = 1).reset_index()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63abb2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk = []\n",
    "\n",
    "for i in range(0, TOTAL):\n",
    "    if df.iloc[i]['dx'] in ['bkl', 'df', 'vasc']:\n",
    "        risk.append('no')\n",
    "    elif df.iloc[i]['dx'] == 'nv':\n",
    "        risk.append('lo')\n",
    "    elif df.iloc[i]['dx'] == 'akiec':\n",
    "        risk.append('md')\n",
    "    elif df.iloc[i]['dx'] in ['mel', 'bcc']:\n",
    "        risk.append('hi')\n",
    "        \n",
    "df['risk'] = risk\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d46b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['image_id'] = df['image_id'].apply(lambda x: x + '.jpg')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b17f6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG-16\n",
    "VGG_load = VGG16(weights='imagenet')\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(VGG_load)\n",
    "model.add(layers.Dense(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f10b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = ImageDataGenerator(\n",
    "    rotation_range=90, \n",
    "    width_shift_range=0.2, \n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True, preprocessing_function = tf.keras.applications.vgg16.preprocess_input,\n",
    "    vertical_flip=True, validation_split=0.1, zoom_range=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29185a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['risk'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ead26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen.flow_from_dataframe(df, directory='./data/HAM10000_images',\n",
    "                                          x_col='image_id', y_col='risk', target_size=(224,224),\n",
    "                                          subset='training').class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a5a030",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen.flow_from_dataframe(df, directory='./data/HAM10000_images',\n",
    "                                          x_col='image_id', y_col='risk', target_size=(224,224),\n",
    "                                          subset='validation').class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3e8065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {'hi':1, 'lo':0.25, 'md':5, 'no':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db25929",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = []\n",
    "\n",
    "for i in range(0, TOTAL):\n",
    "    sample_weights.append(class_weights[df.iloc[i]['risk']])\n",
    "    \n",
    "df['weight'] = sample_weights\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8922b629",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),\n",
    "              loss=tf.nn.softmax_cross_entropy_with_logits,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "    x=data_gen.flow_from_dataframe(df, directory='./data/HAM10000_images', weight_col='weight',\n",
    "                                          x_col='image_id', y_col='risk', target_size=(224,224),\n",
    "                                          subset='training'),\n",
    "    validation_data=data_gen.flow_from_dataframe(df, directory='./data/HAM10000_images', weight_col='weight',\n",
    "                                          x_col='image_id', y_col='risk', target_size=(224,224),\n",
    "                                          subset='validation'),\n",
    "    epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38117486",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = []\n",
    "\n",
    "for i in tqdm(range(0, TOTAL)):\n",
    "    sex = 0 if df.iloc[i]['sex'] == 'male' else 1\n",
    "    age = df.iloc[i]['age']\n",
    "    loc = df.iloc[i]['localization']\n",
    "    \n",
    "    feat = np.array([sex, age])\n",
    "    \n",
    "    if loc == 'abdomen':\n",
    "        feat = np.concatenate((feat, tf.one_hot(0, 15)))\n",
    "    elif loc == 'scalp':\n",
    "        feat = np.concatenate((feat, tf.one_hot(1, 15)))\n",
    "    elif loc == 'lower extremity':\n",
    "        feat = np.concatenate((feat, tf.one_hot(2, 15)))\n",
    "    elif loc == 'trunk':\n",
    "        feat = np.concatenate((feat, tf.one_hot(3, 15)))\n",
    "    elif loc == 'upper extremity':\n",
    "        feat = np.concatenate((feat, tf.one_hot(4, 15)))\n",
    "    elif loc == 'back':\n",
    "        feat = np.concatenate((feat,tf.one_hot(5, 15)))\n",
    "    elif loc == 'neck':\n",
    "        feat = np.concatenate((feat,tf.one_hot(6, 15)))\n",
    "    elif loc == 'face':\n",
    "        feat = np.concatenate((feat,tf.one_hot(7, 15)))\n",
    "    elif loc == 'chest':\n",
    "        feat = np.concatenate((feat,tf.one_hot(8, 15)))\n",
    "    elif loc == 'foot':\n",
    "        feat = np.concatenate((feat,tf.one_hot(9, 15)))\n",
    "    elif loc == 'ear':\n",
    "        feat = np.concatenate((feat,tf.one_hot(10, 15)))\n",
    "    elif loc == 'unknown':\n",
    "        feat = np.concatenate((feat,tf.one_hot(11, 15)))\n",
    "    elif loc == 'hand':\n",
    "        feat = np.concatenate((feat,tf.one_hot(12, 15)))\n",
    "    elif loc == 'acral':\n",
    "        feat = np.concatenate((feat,tf.one_hot(13, 15)))\n",
    "    elif loc == 'genital':\n",
    "        feat = np.concatenate((feat,tf.one_hot(14, 15)))\n",
    "\n",
    "    feature_vector.append(feat)\n",
    "feature_vector = np.array(feature_vector)\n",
    "\n",
    "feature_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea9a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(feature_vector, return_counts=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076d8374",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fc4bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = tf.keras.Model(\n",
    "    inputs=VGG_load.inputs,\n",
    "    outputs=VGG_load.layers[-4].output,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dd9cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = []\n",
    "\n",
    "for i in tqdm(range(0, TOTAL)):\n",
    "    file = df.iloc[i]['image_id']\n",
    "    feat_vec = feature_vector[i]    \n",
    "    \n",
    "    img = image.load_img('./Data/HAM10000_images/' + file, target_size= (224,224))\n",
    "    img = image.img_to_array(img)\n",
    "    # img = np.expand_dims(img, axis=0)\n",
    "    img = tf.keras.applications.vgg16.preprocess_input(img)\n",
    "    img_feat = feature_extractor(img)\n",
    "\n",
    "    preprocessed.append(np.concatenate((img_feat, feat_vec)))\n",
    "    \n",
    "preprocessed = np.array(preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78572b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohr = []\n",
    "\n",
    "for i in range(0, TOTAL):\n",
    "    risk = df.iloc[i]['risk']\n",
    "    \n",
    "    if risk == 'no':\n",
    "        ohr.append(tf.one_hot(0, 4))\n",
    "    elif risk == 'lo':\n",
    "        ohr.append(tf.one_hot(1, 4))\n",
    "    elif risk == 'md':\n",
    "        ohr.append(tf.one_hot(2, 4))\n",
    "    elif risk == 'hi':\n",
    "        ohr.append(tf.one_hot(3, 4))\n",
    "        \n",
    "ohr = np.array(ohr)\n",
    "\n",
    "ohr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ce71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e31ccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = models.Sequential()\n",
    "new_model.add(layers.Dense(4096, activation='relu'))\n",
    "new_model.add(layers.Dense(2048, activation='relu'))\n",
    "new_model.add(layers.Dense(512, activation='relu'))\n",
    "new_model.add(layers.Dense(128, activation='relu'))\n",
    "new_model.add(layers.Dense(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec650b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(optimizer='Adam',\n",
    "              loss=tf.nn.softmax_cross_entropy_with_logits,\n",
    "              metrics=['accuracy'])\n",
    "new_model.fit(preprocessed, ohr, sample_weight=np.array(df['weight']), epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9aaa19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
